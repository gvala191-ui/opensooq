import asyncio

from sendwithbrowser import BrowserSession
import sys
import subprocess


from sendwithbrowser import ensure_playwright


def ensure_dependencies():
    if bool(getattr(sys, "frozen", False)):
        return
    else:
        required = ["curl_cffi", "bs4"]
        for pkg in required:
            try:
                __import__(pkg)
            except ImportError:
                print(f"üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é {pkg}...")
                subprocess.check_call(
                    [sys.executable, "-m", "pip", "install", pkg, "-q"]
                )
                print(f"‚úÖ {pkg} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")


ensure_dependencies()
import curl_cffi
from bs4 import BeautifulSoup
import os
import re
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse, unquote


async def fetch_link(link, proxy=None, cookies=None, headers=None, retries=4, proxy_manager=None) -> str:
    # ***<module>.fetch_link: Failure: Compilation Error
    request_kwargs = {
        "verify": False, 
        "timeout": 30,  # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
        "allow_redirects": True,
        "max_redirects": 2
    }
    
    current_proxy = proxy
    
    if cookies:
        request_kwargs["cookies"] = cookies
    if headers:
        request_kwargs["headers"] = headers
    
    for attempt in range(retries):
        # –ï—Å–ª–∏ –µ—Å—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–∫—Å–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if proxy_manager and hasattr(proxy_manager, 'proxy_list') and proxy_manager.proxy_list:
            current_proxy = proxy_manager.get_next_proxy()
            print(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–∫—Å–∏: {current_proxy.get('http', 'None')}")
        
        if current_proxy:
            request_kwargs["proxies"] = current_proxy
            
        async with curl_cffi.AsyncSession() as s:
            try:
                response = await s.get(link, **request_kwargs)
                if response.status_code == 200:
                    return response.text
                    
                if response.status_code == 403:
                    print(
                        f"‚ö†Ô∏è 403 Forbidden –Ω–∞ {link[:50]}... (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries})"
                    )
                    # –ü–æ–º–µ—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ –∏ –º–µ–Ω—è–µ–º –µ–≥–æ
                    if proxy_manager and current_proxy:
                        proxy_manager.mark_403_error(current_proxy)
                        print("üîÑ –ú–µ–Ω—è—é –ø—Ä–æ–∫—Å–∏ –∏–∑-–∑–∞ 403...")
                        current_proxy = proxy_manager.get_random_proxy()
                    
                    await asyncio.sleep(3)
                    continue
                    
                if response.status_code == 429:
                    print("‚ö†Ô∏è Rate limit 429, –º–µ–Ω—è—é –ø—Ä–æ–∫—Å–∏ –∏ –∂–¥—É 5 —Å–µ–∫...")
                    if proxy_manager and current_proxy:
                        current_proxy = proxy_manager.get_random_proxy()
                    await asyncio.sleep(5)
                    continue
                else:
                    print(f"‚ö†Ô∏è HTTP {response.status_code} –Ω–∞ {link[:50]}...")
                    return

                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å {link[:50]}... –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫")
            except (ConnectionResetError, ConnectionAbortedError, ConnectionError) as e:
                print(f"‚ö†Ô∏è –†–∞–∑—Ä—ã–≤ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries})")
                if proxy_manager and current_proxy:
                    print("üîÑ –ú–µ–Ω—è—é –ø—Ä–æ–∫—Å–∏ –∏–∑-–∑–∞ —Ä–∞–∑—Ä—ã–≤–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è...")
                    current_proxy = proxy_manager.get_random_proxy()
                    
                if attempt < retries - 1:
                    await asyncio.sleep(3)
                    continue
                else:
                    return None
            except Exception as e:
                error_msg = str(e).lower()
                if "connection" in error_msg or "reset" in error_msg or "10054" in error_msg:
                    print(f"‚ö†Ô∏è –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries})")
                    if proxy_manager and current_proxy:
                        current_proxy = proxy_manager.get_random_proxy()
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries}): {e}")
                if attempt < retries - 1:
                    await asyncio.sleep(3)
                else:
                    return None


async def parse_main_page(main_url: str, page: str, proxy: dict) -> list:
    # ***<module>.parse_main_page: Failure: Compilation Error
    headers = {
        "accept": "text/html",
        "accept-language": "ru",
        "accept-encoding": "gzip, deflate, br",  # –î–æ–±–∞–≤–ª—è–µ–º —Å–∂–∞—Ç–∏–µ
        "cache-control": "max-age=0",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    }
    all_links = []
    errors_count = 0
    for page1 in range(1, int(page) + 1):
        try:
            if page1 == 1:
                url = main_url
            else:
                url = f"{main_url}&page={page1}"
            page_content = await fetch_link(url, proxy, None, headers)
            if not page_content:
                print(f"‚ö†Ô∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page1}: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                errors_count += 1
                if errors_count >= 10:
                    print("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫ –ø–æ–¥—Ä—è–¥, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –æ—Å—Ç–∞–≤—à–∏–º–∏—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏")
                    break
                continue
            else:
                soup = BeautifulSoup(page_content, "html.parser")
                elements = soup.find_all(attrs={"data-id1": True})
                links = [elem["data-id1"] for elem in elements]
                all_links.extend(links)
                print(f"‚úì –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page1}: –Ω–∞–π–¥–µ–Ω–æ {len(links)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤.")
                errors_count = 0

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page1}: {e}")
            errors_count += 1
            if errors_count >= 10:
                print("‚ö†Ô∏è –ú–Ω–æ–≥–æ –æ—à–∏–±–æ–∫ –ø–æ–¥—Ä—è–¥, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º")
                break
    return all_links


def parse_user_info(page_html: str) -> dict:
    user_info = {"name": None, "reviews_count": 0}
    if not page_html:
        return user_info
    else:
        try:
            soup = BeautifulSoup(page_html, "html.parser")
            owner_match = re.search(
                "[\"\\']fullName[\"\\']?\\s*:\\s*[\"\\']([^\"\\'\\\\]+)[\"\\']",
                page_html,
            )
            if not owner_match:
                owner_match = re.search(
                    '\\\\?"fullName\\\\?"\\s*:\\s*\\\\?"([^"\\\\]+)\\\\?"', page_html
                )
            if owner_match:
                user_info["name"] = owner_match.group(1)
            ratings_match = re.search(
                "[\"\\']?numberOfRatings[\"\\']?\\s*:\\s*(\\d+)", page_html
            )
            if not ratings_match:
                ratings_match = re.search(
                    '\\\\?"numberOfRatings\\\\?"\\s*:\\s*(\\d+)', page_html
                )
            if ratings_match:
                user_info["reviews_count"] = int(ratings_match.group(1))
            if user_info["reviews_count"] == 0:
                rating_label = soup.find("label", string=re.compile("ÿßŸÑÿ™ŸÇŸäŸäŸÖ"))
                if rating_label:
                    parent_div = rating_label.find_parent(
                        "div", class_=re.compile("flex")
                    )
                    if parent_div:
                        buttons = parent_div.find_all(
                            "button", class_=re.compile("text-primary")
                        )
                        for btn in buttons:
                            btn_text = btn.get_text(strip=True)
                            match = re.search("\\(\\s*(\\d+)\\s*\\)", btn_text)
                            if match:
                                user_info["reviews_count"] = int(match.group(1))
                                break
            if not user_info["name"]:
                owner_section = soup.find("section", id="ListingViewListingOwner")
                if owner_section:
                    member_link = owner_section.find(
                        "a", href=re.compile("/ar/mid/member-")
                    )
                    if member_link:
                        name_elem = owner_section.find(
                            "a", class_=re.compile("font-bold")
                        )
                        if name_elem:
                            user_info["name"] = name_elem.get_text(strip=True)
                    if user_info["reviews_count"] == 0:
                        rating_button = owner_section.find(
                            "button", class_=re.compile("text-primary")
                        )
                        if rating_button:
                            reviews_text = rating_button.get_text(strip=True)
                            match = re.search("\\(\\s*(\\d+)\\s*\\)", reviews_text)
                            if match:
                                user_info["reviews_count"] = int(match.group(1))
            return user_info
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ user_info: {e}")
            return user_info


def func_proxy() -> dict:
    # ***<module>.func_proxy: Failure: Compilation Error
    try:
        proxy_host_port = input(
            "–í–≤–µ–¥–∏—Ç–µ –ø—Ä–æ–∫—Å–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ host:port (–∏–ª–∏ Enter –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞): "
        ).strip()
        if not proxy_host_port:
            print("‚ö†Ô∏è –ü—Ä–æ–∫—Å–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.")
            return
        else:
            proxy = {
                "http": f"http://{proxy_host_port}",
                "https": f"http://{proxy_host_port}",
            }

        response = curl_cffi.get(
            "https://httpbin.org/ip", proxies=proxy, verify=False, timeout=10
        )
        if response.status_code == 200:
            ip = response.json().get("origin", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
            print(f"‚úì –ü—Ä–æ–∫—Å–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç! IP: {ip}")
            return proxy
        else:
            print(f"‚ö†Ô∏è –ü—Ä–æ–∫—Å–∏ –æ—Ç–≤–µ—Ç–∏–ª –∫–æ–¥–æ–º {response.status_code}")
            retry = input("–ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π? (y/n): ").strip().lower()
            if retry == "y":
                return func_proxy()
            else:
                return proxy
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–∫—Å–∏: {e}")
        retry = input("–ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π? (y/n): ").strip().lower()
        if retry == "y":
            return func_proxy()

    return func_proxy()


def load_cookies() -> dict:
    try:
        with open("cookies.txt", "r", encoding="utf-8") as file:
            cookies_list = json.load(file)
        cookies_dict = {}
        for cookie in cookies_list:
            cookies_dict[cookie["name"]] = cookie["value"]
        return cookies_dict
    except FileNotFoundError:
        print("–§–∞–π–ª cookies.txt –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        asd = input("–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –≤—ã—Ö–æ–¥–∞...")
        os._exit(1)
    except json.JSONDecodeError:
        print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ cookies.txt. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç JSON.")
        asd = input("–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –≤—ã—Ö–æ–¥–∞...")
        os._exit(1)


def load_blacklist() -> set:
    try:
        with open("blacklist.txt", "r", encoding="utf-8") as file:
            blacklist = set(
                (
                    line.strip().lower()
                    for line in file
                    if line.strip() and (not line.startswith("#"))
                )
            )
        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(blacklist)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –±–ª–µ–∫–ª–∏—Å—Ç–∞")
        return blacklist
    except FileNotFoundError:
        print("‚ö† –§–∞–π–ª blacklist.txt –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞—é –Ω–æ–≤—ã–π...")
        with open("blacklist.txt", "w", encoding="utf-8") as file:
            file.write(
                "# –ë–ª–µ–∫–ª–∏—Å—Ç - –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ (–∫–∞–∂–¥—ã–π —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)\n"
            )
        return set()


def add_to_blacklist(identifier: str):
    # ***<module>.add_to_blacklist: Failure: Compilation Error
    if not identifier:
        return
    existing = load_blacklist()
    if identifier.lower() in existing:
        return
    try:
        with open("blacklist.txt", "a", encoding="utf-8") as file:
            file.write(f"{identifier}\n")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –±–ª–µ–∫–ª–∏—Å—Ç: {e}")
        return None


def is_in_blacklist(identifier: str, blacklist: set) -> bool:
    if not identifier:
        return False
    else:
        return identifier.lower() in blacklist


async def parse_link(link: str, proxy: dict, cookies: str) -> dict:
    headers = {
        "accept": "text/html",
        "accept-language": "ru-RU,ru;q=0.9",
        "accept-encoding": "gzip, deflate, br",  # –î–æ–±–∞–≤–ª—è–µ–º —Å–∂–∞—Ç–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç—Ä–∞—Ñ–∏–∫–∞
        "cache-control": "max-age=0",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    }
    try:
        page_content = await fetch_link(link, proxy, cookies, headers)
        if not page_content:
            return {"name": None, "reviews_count": 0}

        # –£–±–∏—Ä–∞–µ–º –≤—ã–≤–æ–¥ —Ä–∞–∑–º–µ—Ä–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        # print(f"–†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {len(page_content)}")

        user_info = parse_user_info(page_content)
        if user_info["name"]:
            print(f"‚úì –ù–∞–π–¥–µ–Ω: {user_info['name']}, –æ—Ç–∑—ã–≤–æ–≤: {user_info['reviews_count']}")
        return user_info
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ parse_link: {e}")
        return {"name": None, "reviews_count": 0}


async def parse_user_thread(
    id: str,
    proxy: dict,
    cookies: dict,
    max_reviews: int,
    blacklist: set,
    base_domain: str,
) -> dict:
    # ***<module>.parse_user_thread: Failure: Compilation Error
    link = f"https://{base_domain}/ar/search/{id}"
    try:
        if is_in_blacklist(id, blacklist):
            return
        user_data = await parse_link(link, proxy, cookies)
        if not user_data or not user_data.get("name"):
            return None
        if is_in_blacklist(user_data["name"], blacklist):
            return None
        if user_data["reviews_count"] <= max_reviews:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –±–ª–µ–∫–ª–∏—Å—Ç —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞
            add_to_blacklist(id)
            add_to_blacklist(user_data["name"])
            return {
                "id": id,
                "name": user_data["name"],
                "reviews_count": user_data["reviews_count"],
                "link": link,
            }

    except Exception as e:
        print(e)
        return None


async def send_messages_to_user_with_session(
    browser_session, user: dict, image_path: str, sends_count: int = 5
):
    chat_link = user.get("chat_link")
    if not chat_link or chat_link in ("–ù–µ –¥–æ—Å—Ç—É–ø–µ–Ω", "–û—à–∏–±–∫–∞"):
        return False
    else:
        print(f"‚Üí {user['name']}")
        success_count = 0
        for i in range(sends_count):
            try:
                result = await browser_session.send_image(chat_link, image_path)
                if result:
                    success_count += 1
                    print(f"  ‚úì [{i + 1}/{sends_count}]")
                else:
                    print(f"  ‚úó [{i + 1}/{sends_count}]")
                if i < sends_count - 1:
                    await asyncio.sleep(1.5)
            except Exception as e:
                print(f"  ‚úó –û—à–∏–±–∫–∞: {e}")
        if success_count > 0:
            add_to_blacklist(user["name"])
            add_to_blacklist(user["id"])
        return success_count > 0


from datetime import datetime


async def parse_for_message(link: str, proxy: dict, cookies: str) -> str:
    try:
        headers = {
            "accept": "text/html",
            "accept-encoding": "gzip, deflate, br",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        }
        page_content = await fetch_link(link, proxy, cookies, headers)
        if not page_content:
            return None

        # 1. –î–æ—Å—Ç–∞–µ–º listing_id (postId)
        listing_id = None
        l_match = re.search(r'\\"postId\\":(\d+)', page_content)
        if l_match: listing_id = l_match.group(1)

        # 2. –î–æ—Å—Ç–∞–µ–º member_id (ID –≤–ª–∞–¥–µ–ª—å—Ü–∞ –≤ ownerData)
        member_id = None
        m_match = re.search(r'\\"ownerData\\":\{\\"id\\":(\d+)', page_content)
        if m_match: member_id = m_match.group(1)

        # 3. –î–æ—Å—Ç–∞–µ–º member_data_id (—ç—Ç–æ id –≤–Ω—É—Ç—Ä–∏ memberData –≤ –±–ª–æ–∫–µ listings)
        # –í —Ç–≤–æ–µ–º –¥–∞–º–ø–µ –æ–Ω —á–∞—Å—Ç–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å member_id, –Ω–æ –º—ã –∏—â–µ–º –µ–≥–æ –æ—Ç–¥–µ–ª—å–Ω–æ
        member_data_id = member_id  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –±–µ—Ä–µ–º —ç—Ç–æ—Ç, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–º –¥—Ä—É–≥–æ–π
        md_match = re.search(r'\\"member_id\\":\\"(\d+)\\"', page_content)
        user_info_cookie = cookies.get('userInfo')
        if user_info_cookie:
            try:
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º %7B%22id... –≤ {"id":...
                decoded_str = unquote(user_info_cookie)
                user_data = json.loads(decoded_str)
                if 'id' in user_data:
                    my_id = str(user_data['id'])
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å userInfo: {e}")

        if listing_id and member_id and member_data_id:
            today = datetime.now().strftime("%Y-%m-%d")
            # –°–æ–±–∏—Ä–∞–µ–º —Ç—É —Å–∞–º—É—é —Å—Å—ã–ª–∫—É

            chat_url = (
                f"https://my.opensooq.com/chats/open/{listing_id}/{member_id}/{my_id}"
                f"?cSource=opensooq&cMedium=none&cName=direct_web_open&v={today}&selectedRoom={listing_id}-{member_id}-{my_id}"
            )
            print(f"‚úì –°—Å—ã–ª–∫–∞ —Å–æ–±—Ä–∞–Ω–∞: {listing_id}")
            return chat_url

        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –≤—Å–µ ID –¥–ª—è —Å—Å—ã–ª–∫–∏: L:{listing_id} M:{member_id} MD:{member_data_id}")
        return None

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∫–∏ —Å—Å—ã–ª–∫–∏: {e}")
        return None


async def get_chat_link_thread(user: dict, proxy: dict, cookies: dict) -> dict:
    try:
        chat_link = await parse_for_message(user["link"], proxy, cookies)
        user["chat_link"] = chat_link if chat_link else "–ù–µ –¥–æ—Å—Ç—É–ø–µ–Ω"
        return user
    except Exception:
        user["chat_link"] = "–û—à–∏–±–∫–∞"
        return user


async def parser() -> list:
    await ensure_playwright()
    # ***<module>.parser: Failure: Different control flow
    cookie_dict = load_cookies()
    blacklist = load_blacklist()


    main_link = input('–í–≤–µ–¥–∏ —Å—Å—ã–ª–∫—É: ')
    #main_link = "https://ae.opensooq.com/en/find?search=true&sort_code=recent"
    max_reviews = int(input('–í–≤–µ–¥–∏ –º–∞–∫—Å –æ—Ç–∑—ã–≤–æ–≤: '))
    #max_reviews = 10
    while True:
        try:
            page, sends = input('–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–ø—Ä–∞–≤–æ–∫ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1,10): ').split(',')
            #page, sends = 1, 4
            sends = int(sends.strip())
            page = int(page.strip())
        except:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –≤–≤–æ–¥–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
        else:
            print(f"\n=== –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–æ—Ç–æ ===")
            # –ò—â–µ–º —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–∞–ø–∫–µ
            image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']
            image_file = None
            for file in os.listdir('.'):
                if any(file.lower().endswith(ext) for ext in image_extensions):
                    image_file = os.path.abspath(file)
                    break
            
            if not image_file:
                print(f"\n‚ùå –û—à–∏–±–∫–∞: —Ñ–æ—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!")
                print(f"–î–æ–±–∞–≤—å—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (.jpg, .png –∏ —Ç.–¥.) –≤ –ø–∞–ø–∫—É —Å –ø—Ä–æ–≥—Ä–∞–º–º–æ–π.")
                asd = input("–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –≤—ã—Ö–æ–¥–∞...")
                os._exit(1)
            
            if os.path.exists(image_file):
                file_size = os.path.getsize(image_file) / 1024  # –≤ KB
                print(f"‚úì –ù–∞–π–¥–µ–Ω–æ —Ñ–æ—Ç–æ: {os.path.basename(image_file)} ({file_size:.1f} KB)")
            else:
                print(f"\n‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {image_file} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
                asd = input("–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –≤—ã—Ö–æ–¥–∞...")
                os._exit(1)
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø—Ä–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –∑–∞–ø—É—Å–∫–µ
            threads_count = 40  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è 20 –ú–±–∏—Ç/—Å –ø—Ä–∏ 2-4 —Å–æ—Ñ—Ç–∞—Ö
            proxy = None
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–∫—Å–∏: {proxy}" if proxy else "–ü—Ä–æ–∫—Å–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.")

            print(f"\n{'============================================================'}")
            print("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞:")
            print(f"  –°—Å—ã–ª–∫–∞: {main_link}")
            print(f"  –°—Ç—Ä–∞–Ω–∏—Ü: {page}")
            print(f"  –ú–∞–∫—Å –æ—Ç–∑—ã–≤–æ–≤: {max_reviews}")
            print(f"  –°–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {sends}")
            print(f"  –ü–æ—Ç–æ–∫–æ–≤: {threads_count}")
            print(f"{'============================================================'}\n")
            parsed_url = urlparse(main_link)
            base_domain = parsed_url.netloc
            print(f"üåç –ì–µ–æ –¥–æ–º–µ–Ω: {base_domain}")
            print("\n=== –ü–∞—Ä—Å–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π ===")
            ids = await parse_main_page(main_link, page, proxy)
            print(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(ids)}")
            print(f"\n=== –ü–∞—Ä—Å–∏–Ω–≥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π ({threads_count} –ø–æ—Ç–æ–∫–æ–≤) ===")
            all_users_data = []
            semaphore = asyncio.Semaphore(threads_count)
            all_users_data = []

            async def sem_task(user_id):
                async with semaphore:
                    await asyncio.sleep(0.15)  # –£–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
                    result = await parse_user_thread(
                        user_id, proxy, cookie_dict, max_reviews, blacklist, base_domain
                    )
                    if result:
                        all_users_data.append(result)

            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á
            tasks = [sem_task(user_id) for user_id in ids]

            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å—ë –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            await asyncio.gather(*tasks)
            tasks.clear()
            print("\n‚úì –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω!")
            print(
                f"–í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –æ—Ç–∑—ã–≤–∞–º–∏ ‚â§ {max_reviews}: {len(all_users_data)}"
            )
            print(f"\n=== –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ —á–∞—Ç—ã ({threads_count} –ø–æ—Ç–æ–∫–æ–≤) ===")
            updated_users = []
            completed = 0
            total = len(all_users_data)
            async def worker(user):
                async with semaphore:
                    await asyncio.sleep(0.15)  # –£–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    result = await get_chat_link_thread(user, proxy, cookie_dict)
                    if result:
                        updated_users.append(result)

            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–¥–∞—á
            tasks = [worker(user) for user in all_users_data]

            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å—ë –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏ –∂–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            await asyncio.gather(*tasks)
            
            print(f"\n‚úì –ü–æ–ª—É—á–µ–Ω–æ {len(updated_users)} —á–∞—Ç-—Å—Å—ã–ª–æ–∫")
            with open("results.txt", "w", encoding="utf-8") as f:
                for user in updated_users:
                    f.write(
                        f"ID: {user['id']} | –ò–º—è: {user['name']} | –û—Ç–∑—ã–≤–æ–≤: {user['reviews_count']}\n"
                    )
                    f.write(f"–û–±—ä—è–≤–ª–µ–Ω–∏–µ: {user['link']}\n")
                    f.write(f"–ß–∞—Ç: {user.get('chat_link', '–ù–µ –¥–æ—Å—Ç—É–ø–µ–Ω')}\n")
                    f.write(
                        "--------------------------------------------------------------------------------\n"
                    )
            print("\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ results.txt")
            print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(updated_users)}")
            send_messages = "y"
            if send_messages in ["yes", "y", "–¥–∞", "–¥"]:
                print(
                    f"\n=== –û—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–æ—Ç–æ ({len(updated_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π) ==="
                )
                print(f"–ë—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –ø–æ {sends} —Ñ–æ—Ç–æ –∫–∞–∂–¥–æ–º—É")
                print("üöÄ –ó–∞–ø—É—Å–∫–∞—é –±—Ä–∞—É–∑–µ—Ä...")
                browser = await BrowserSession(proxy=proxy).start()
                sent_count = 0
                failed_count = 0
                try:
                    for i, user in enumerate(updated_users, 1):
                        print(f"\n[{i}/{len(updated_users)}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {user['name']}")
                        result = await send_messages_to_user_with_session(
                            browser, user, image_file, sends
                        )
                        if result:
                            sent_count += 1
                        else:
                            failed_count += 1
                finally:
                   await browser.close()
                print(
                    f"\n{'============================================================'}"
                )
                print("–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
                print(f"–£—Å–ø–µ—à–Ω–æ: {sent_count}/{len(updated_users)}")
                print(f"–ù–µ—É–¥–∞—á–Ω–æ: {failed_count}/{len(updated_users)}")
                return updated_users
            else:
                print("\n‚äó –û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –ø—Ä–æ–ø—É—â–µ–Ω–∞.")
                return updated_users


if __name__ == '__main__':
    asyncio.run(parser())